{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_Example_Ames_Housing_Dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/titsitits/Python_Data_Science/blob/master/8_Example_Ames_Housing_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfQ-d2onvooo",
        "colab_type": "text"
      },
      "source": [
        "Mickaël Tits\n",
        "CETIC\n",
        "mickael.tits@cetic.be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvc0ah8oge9x",
        "colab_type": "text"
      },
      "source": [
        "# Ames (Iowa) Housing Dataset\n",
        "\n",
        "Plus d'informations ici:\n",
        "http://jse.amstat.org/v19n3/decock.pdf\n",
        "\n",
        "Détails sur les variables: https://github.com/titsitits/Python_Data_Science/blob/master/Donn%C3%A9es/data_description.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO_L0DC-QxU9",
        "colab_type": "text"
      },
      "source": [
        "## Préparation/exploration du dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rW_gAdoCDOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "ames = pd.read_csv(\"https://raw.githubusercontent.com/titsitits/Python_Data_Science/master/Donn%C3%A9es/train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LCRRtGRju_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ames.count().values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsjzxK-2Yh8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#another visualization library\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(rc={'figure.figsize':(5,5)})\n",
        "sns.distplot(ames.SalePrice, bins=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2wlMqHH6Een",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_ames, val_ames = train_test_split(ames, test_size = 0.2, random_state=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb39u7x-7Gg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(train_ames.SalePrice, bins=30)\n",
        "sns.distplot(val_ames.SalePrice, bins=30)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m308qsX1pXLP",
        "colab_type": "text"
      },
      "source": [
        "## Exploration de base: analyse de corrélation (variables continues)\n",
        "\n",
        "Remarque: la méthode pandas.DataFrame.corr() omet automatiquement les variables non-numériques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdC4Ay36fqTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "correlation_matrix = train_ames.corr()\n",
        "\n",
        "#corrélations des variables avec le prix\n",
        "corr_with_price = correlation_matrix[\"SalePrice\"]\n",
        "\n",
        "#On trie les variables selon la valeur absolue de leur corrélation avec le prix\n",
        "best_features = corr_with_price.abs().sort_values(ascending=False)\n",
        "\n",
        "# On ne évidemment peut utiliser le label comme variable prédictive\n",
        "best_features = best_features.drop(\"SalePrice\")\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "best_features.plot.bar()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLzQ8Tbc71mW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig,axes = plt.subplots(1,2, figsize=(10,5))\n",
        "ames.plot.scatter(\"OverallQual\",\"SalePrice\", ax = axes[0])\n",
        "ames.plot.scatter(\"GrLivArea\",\"SalePrice\", ax = axes[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXC6Cbon8UE2",
        "colab_type": "text"
      },
      "source": [
        "Note: On remarque que deux points pourraient être considérés comme des outliers: ils s'écartent fortement de la relation linéaire globale entre les deux variables. Une question qui se pose naturellement: A quels points du premier graphe correspondent ceux du deuxième graphe (en particulier les outliers) ?\n",
        "\n",
        "La librairie moderne de visualition graphique **`atlair`** permet de répondre à ce genre de question.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXGqQjXbPPR5",
        "colab_type": "text"
      },
      "source": [
        "### Bonus :  visualisation moderne (interactive) avec `atlair`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu_lxYSv9YD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load an example dataset\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "interval = alt.selection_interval()\n",
        "\n",
        "chart = alt.Chart(ames).mark_point().encode(  y='SalePrice',\n",
        "  color=alt.condition(interval, 'SaleCondition', alt.value('lightgray')) ).properties( selection=interval)\n",
        "\n",
        "chart.encode(x='GrLivArea') | chart.encode(x='OverallQual')\n",
        "\n",
        "#Les graphiques ci-dessous\n",
        "\n",
        "#N'hésitez pas à tester une autre catégorie (au lieu de MSZoning) pour définir la légende (par exemple: SaleCondition )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdQIz2n5_k7w",
        "colab_type": "text"
      },
      "source": [
        "Les graphiques ci-dessus sont interactifs. Vous pouvez sélectionner une partie des points avec votre souris. \n",
        "On remarque que les deux outliers en terme de ground_living_area sont le même qu'en terme de overall_quality. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_So3t3pFfCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_outlier = (ames[\"GrLivArea\"] > 4000) & (ames[\"SalePrice\"] < 300000)\n",
        "display(ames[is_outlier == True])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L04XgWlqGxJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mode: valeur la plus fréquente\n",
        "ames.mode().iloc[0:1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMAhx9XcHiWW",
        "colab_type": "text"
      },
      "source": [
        "Une comparaison rapide entre les outliers et les catégories les plus fréquentes montrent notamment: les outliers sont construits sur des terrains irréguliers (LotShape = IR3, et LandContour = Lvl), et les bâtiments ne sont pas finis (SaleType = New, SaleCondition = Partial). Ces particularités peuvent expliquer leur prix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GieK4cCJQecG",
        "colab_type": "text"
      },
      "source": [
        "## Préparation des sets d'entraînement et de validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyTRzQZHNenu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#garder les outliers ou pas ?\n",
        "\n",
        "ames2 = ames[~is_outlier]\n",
        "\n",
        "#ames2 = ames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZcr_oCYSmm9",
        "colab_type": "text"
      },
      "source": [
        "### Sélection des variables prédictives (features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjOypw1hSXD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prenons les cinq meilleures variables prédictives (en sa basant sur leur corrélation avec le prix)\n",
        "featurelist = best_features.index[:5].to_list()\n",
        "\n",
        "dataset = ames2[featurelist+[\"SalePrice\"]]\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "X = dataset[featurelist]\n",
        "y = dataset[\"SalePrice\"]\n",
        "\n",
        "dataset.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCqFx1_6Sq92",
        "colab_type": "text"
      },
      "source": [
        "### Séparation des données d'entraînement et de validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDKVXwL9m5l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laD_Mgz0S4d1",
        "colab_type": "text"
      },
      "source": [
        "## Régression linéaire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pij4mZ5l32w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "price_predictor = LinearRegression()\n",
        "price_predictor.fit(Xtrain, ytrain)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4wMp6XSmJFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "print(\"R2 Scores (train, val):\", price_predictor.score(Xtrain, ytrain), price_predictor.score(Xval, yval))\n",
        "\n",
        "\n",
        "y_pred = price_predictor.predict(Xval)\n",
        "\n",
        "print(\"biais:\", np.mean(y_pred - yval.values) ) #si il est négatif: sous-évaluation (en moyenne), si il est positif: sur-évaluation\n",
        "print(\"MAE:\", mean_absolute_error(yval.values, y_pred) )\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(yval.values, y_pred)) )\n",
        "\n",
        "#Tester en gardant ou en retirant les outliers; tester avec différentes ensembles de variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oTHcBp-Svi3",
        "colab_type": "text"
      },
      "source": [
        "## Comparaison pour différents nombres de variables prédictives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0Rd7RnFo57A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perfs = pd.DataFrame(columns = ['train','val'])\n",
        "\n",
        "for n in range(1,len(best_features)):\n",
        "\n",
        "  featurelist = best_features.index[:n].to_list()\n",
        "  \n",
        "  dataset = ames2[featurelist+[\"SalePrice\"]]\n",
        "  dataset = dataset.dropna()\n",
        "  \n",
        "  X = dataset[featurelist]\n",
        "  y = dataset[\"SalePrice\"]\n",
        "  \n",
        "  Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "  price_predictor.fit(Xtrain, ytrain)\n",
        "  perfs.loc[n] = price_predictor.score(Xtrain, ytrain), price_predictor.score(Xval, yval)\n",
        "\n",
        "perfs.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtnVoUtXW5Dh",
        "colab_type": "text"
      },
      "source": [
        "On remarque que les performances en validation grimpent fortement lors de l'ajout de la 11ème variable, et tombent lors de l'ajout de la 15ème. Vérifions les performances en omettant la 15ème variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnOn54BEWkrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(best_features.index[14])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dswF-30GVnAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featurelist = best_features.index.to_list()\n",
        "\n",
        "featurelist.remove('LotFrontage')\n",
        "featurelist.remove('Id')\n",
        "\n",
        "dataset = ames2[featurelist+[\"SalePrice\"]]\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "X = dataset[featurelist]\n",
        "y = dataset[\"SalePrice\"]\n",
        "\n",
        "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "price_predictor.fit(Xtrain, ytrain)\n",
        "\n",
        "price_predictor.score(Xtrain, ytrain), price_predictor.score(Xval, yval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx-6wplugkaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(yval,price_predictor.predict(Xval))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uugb_WZ_hh8D",
        "colab_type": "text"
      },
      "source": [
        "On constate que le nuage de doit semble former une légère courbe. Cela semble indiquer qu'il existe des relations non-linéaires, telles que des relations polynomiales entre les variables et le prix. La régression linéaire se base en effet sur l'hypothèse très simpliste que les relations seraient linéaires. Si on revisualise les nuages de points plus haut montrant l'interactions entre différentes variables (comme 'OverallQual') et le prix, on constate que la relation et plutôt quadratique que linéaire.\n",
        "\n",
        "Une variation de la régression linéaire, appelée régression polynomiale, permet de facilement prendre en compte de potentielles interactions non-linéaires entre les variables, en créant de nouvelles variables à partir de polynômes des variables d'origine. Bien sûr, les possibilités ne se liminent pas aux polynômes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAI-or-klHpt",
        "colab_type": "text"
      },
      "source": [
        "##Régression polynomiale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgiED5-Ug39a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "X = dataset[featurelist]\n",
        "\n",
        "def add_polynomials(X):\n",
        "  for col in X.columns:\n",
        "    #prise en compte des carrés des variables\n",
        "    X[col+\"_square\"] = X[col]**2\n",
        "    #prise en compte des racines carrées\n",
        "    X[col+\"_sqrt\"] = X[col]**(1/2)\n",
        "    \n",
        "  return X\n",
        "\n",
        "X = add_polynomials(X)\n",
        "\n",
        "len(X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cp45O_3hEqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "price_predictor.fit(Xtrain, ytrain)\n",
        "\n",
        "print(\"performances (train,val):\", price_predictor.score(Xtrain, ytrain), price_predictor.score(Xval, yval))\n",
        "plt.scatter(yval,price_predictor.predict(Xval))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZoJLuVkVIRx",
        "colab_type": "text"
      },
      "source": [
        "## Comparaison pour différents nombres d'exemples (taille du training set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q74RICIET8YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perfs2 = pd.DataFrame(columns = ['train','val'])\n",
        "\n",
        "for nsamples in range(10,800,10):\n",
        "  \n",
        "  trainset, valset = train_test_split(dataset, test_size = 550, random_state=5)\n",
        "  \n",
        "  #On extrait un sample sur le training set\n",
        "  trainset_sample = trainset.sample(nsamples)\n",
        "  \n",
        "  Xtrain = trainset_sample[featurelist]\n",
        "  ytrain = trainset_sample[\"SalePrice\"]\n",
        "  \n",
        "  Xval = valset[featurelist]\n",
        "  yval = valset[\"SalePrice\"]\n",
        "  \n",
        "  price_predictor.fit(Xtrain, ytrain)\n",
        "  perfs2.loc[nsamples] = price_predictor.score(Xtrain, ytrain), price_predictor.score(Xval, yval)\n",
        "  \n",
        "perfs2.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atnp5Lo6aaLe",
        "colab_type": "text"
      },
      "source": [
        "On constate que les performances diminuent avec le nombre d'échantillons d'entraînement, et que les performances en validation sont au départ beaucoup plus faibles, ce qui est signe d'**overfitting**. En effet, en utilisant seulement 50 exemples (gauche du graphe), on remarque que le modèle compte presque autant de paramètres (un par variable prédictive, soit 36). Dans ce cas, les paramètres peuvent être adaptés aux exemples spécifiques d'entraînement pour apprendre leur prédiction presque \"par coeur\". Le modèle n'est alors pas générique et fonctionne mal sur de nouvelles données (comme le montrent les faibles performances en validation). On constate ensuite que pour 400 échantillons d'entraînement, les performances en entraînement se stabilisent, et les performances en validation sont équivalentes, ce qui signifie que le nombre d'échantillons est suffisant (plus d'overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6tK_SqBZZY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(price_predictor.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL63VJR3c7fu",
        "colab_type": "text"
      },
      "source": [
        "## Performances réelles\n",
        "\n",
        "Pour évaluer les performances réelles du modèles, nous pouvons utiliser un set de données qui n'a pas encore été utilisé, ni pour l'entraînement ni pour la validation. Cela permet de simuler une utilisation réelle du modèle, sur de nouvelles maisons (dont on voudrait estimer le prix). L'évaluation des performances à partir du set de validation seraient biaisée, car il a en effet permis de choisir les paramètres du modèles, en l'occurence le choix des variables prédictives (le modèle a donc été indirectement optimisé pour ces données spécifiques).\n",
        "\n",
        "Sur la plateforme [Kaggle](https://www.kaggle.com/), pour garantir l'absence de biais lors du design du modèle, un set de test est généralement fourni séparément, et les labels ne sont volontairement pas fournis. L'utilisateur de la plateforme doit alors soumettre à la plateforme les prédictions données par le modèle développé, et reçoit le résultat de l'évaluation effectuée par la plateforme. Cela permet notamment de classer différents compétiteurs lors d'un concours: https://www.kaggle.com/competitions\n",
        "\n",
        "Concernant le dataset utilisé, on peut soumettre des prédictions ici: https://www.kaggle.com/c/home-data-for-ml-course/overview/evaluation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1nRGO6XeQY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Métrique utilisée sur Kaggle: \n",
        "#Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. \n",
        "#(Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n",
        "\n",
        "def metric(a,b):\n",
        "  \n",
        "  a = np.log(a)\n",
        "  b = np.log(b)\n",
        "  return np.sqrt(mean_squared_error(a,b))\n",
        "\n",
        "#Mean absolute error (plus intuitif)\n",
        "def MAE(a,b):\n",
        "  return mean_absolute_error(a,b)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5eYYBMhTQJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vérifions déjà les performances sur le set de validation\n",
        "\n",
        "featurelist = best_features.index[:n].to_list()\n",
        "\n",
        "dataset = ames[featurelist+[\"SalePrice\"]]\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "X = dataset[featurelist]\n",
        "\n",
        "y = dataset[\"SalePrice\"]\n",
        "\n",
        "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.4)\n",
        "\n",
        "price_predictor.fit(Xtrain, ytrain)\n",
        "\n",
        "trainpreds = price_predictor.predict(Xtrain)\n",
        "valpreds = price_predictor.predict(Xval)\n",
        "\n",
        "#Pour éviter de prédire des valeurs anormales, on limite les prédictions au range du set d'entraînement\n",
        "trainpreds = trainpreds.clip(ytrain.min(), ytrain.max())\n",
        "valpreds = valpreds.clip(ytrain.min(), ytrain.max())\n",
        "\n",
        "\n",
        "MAE(trainpreds,ytrain.values), MAE(valpreds,yval.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_tHgbL2p_mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uncomment to use polynomials\n",
        "X = dataset[featurelist]\n",
        "X = add_polynomials(X)\n",
        "\n",
        "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.4)\n",
        "\n",
        "poly_predictor = LinearRegression()\n",
        "poly_predictor.fit(Xtrain, ytrain)\n",
        "\n",
        "trainpreds = poly_predictor.predict(Xtrain)\n",
        "valpreds = poly_predictor.predict(Xval)\n",
        "\n",
        "#Pour éviter de prédire des valeurs anormales, on limite les prédictions au range du set d'entraînement\n",
        "trainpreds = trainpreds.clip(ytrain.min(), ytrain.max())\n",
        "valpreds = valpreds.clip(ytrain.min(), ytrain.max())\n",
        "\n",
        "\n",
        "metric(trainpreds,ytrain.values), metric(valpreds,yval.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2chtMCm7-T7k",
        "colab_type": "text"
      },
      "source": [
        "Remarquez que vous obtiendrez des résultats différents à chaque fois que vous relancez les cellules ci-dessus: cela est dû à la séparation aléatoire des sets d'entraînement et de validation. \n",
        "\n",
        "Afin d'avoir un indicateur plus robuste du modèle, une possibilité serait de calculer une moyenne de ces résultats, ou d'utiliser un processus **leave-one-out** (voir Chapitre précédent)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSvjBcA7bEi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset = pd.read_csv(\"https://raw.githubusercontent.com/titsitits/Python_Data_Science/master/Donn%C3%A9es/test.csv\")\n",
        "\n",
        "testset = testset[featurelist].fillna(0)\n",
        "testpreds = price_predictor.predict(testset)\n",
        "\n",
        "testpreds = testpreds.clip(ytrain.min(), ytrain.max())\n",
        "\n",
        "submission = testset\n",
        "submission[\"SalePrice\"] = testpreds\n",
        "submission = submission[[\"Id\",\"SalePrice\"]]\n",
        "submission.to_csv(\"mysubmission_linear_regression.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkM2iUzEpspz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset = pd.read_csv(\"https://raw.githubusercontent.com/titsitits/Python_Data_Science/master/Donn%C3%A9es/test.csv\")\n",
        "\n",
        "testset = testset[featurelist].fillna(0)\n",
        "\n",
        "testset2 = add_polynomials(testset)\n",
        "testpreds = poly_predictor.predict(testset2)\n",
        "\n",
        "testpreds = testpreds.clip(ytrain.min(), ytrain.max())\n",
        "\n",
        "submission = testset\n",
        "submission[\"SalePrice\"] = testpreds\n",
        "submission = submission[[\"Id\",\"SalePrice\"]]\n",
        "submission.to_csv(\"mysubmission_polynomial_regression.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23u9i5kTpKME",
        "colab_type": "text"
      },
      "source": [
        "## Exploration de base des variables catégorielles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMD_aKxojyyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Analysons l'effet de chaque variable catégorielle\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "all_columns = ames.columns\n",
        "cont_columns = featurelist\n",
        "\n",
        "cat_columns = set(all_columns) - set(cont_columns) - set(['LotFrontage'])\n",
        "cat_columns = list(cat_columns)\n",
        "cat_columns.remove(\"SalePrice\")\n",
        "\n",
        "diffs_per_group = []\n",
        "\n",
        "for cat in cat_columns:  \n",
        "  \n",
        "  group_means = ames.groupby(cat)[\"SalePrice\"].mean()\n",
        "  diffs_per_group.append(group_means.max() - group_means.min())\n",
        "  \n",
        "  #Bonus: vous pouvez afficher les graphes pour chaque groupe\n",
        "  #plt.figure(figsize=(5,5))\n",
        "  #group_means.plot.bar()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Gc_Hd9olCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diffs_per_group = np.array(diffs_per_group)\n",
        "best_cats = diffs_per_group.argsort()[::-1]\n",
        "best_cats_list = [cat_columns[i] for i in best_cats]\n",
        "print(best_cats_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dII0ByqLute_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ames[\"PoolQC\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MGOGd4Xu058",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ames[\"PoolQC\"] = ames[\"PoolQC\"].fillna(\"NP\") #no pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXKxZKudqIlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ames.groupby(\"PoolQC\")[\"SalePrice\"].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIoTezR8R3kr",
        "colab_type": "text"
      },
      "source": [
        "## Bonus - Un algorithme plus moderne: catboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdEdNo8uPa7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installer un nouveau package: on appelle une ligne de commande linux grâce au symbole \"!\". On utilise le gestionnaire de packages python pip pour installer un nouveau package\n",
        "!pip install catboost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDhe159xjnZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contfeatures = featurelist#[:10]\n",
        "catfeatures = best_cats_list#[:10]\n",
        "\n",
        "newfeaturelist =  contfeatures + catfeatures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIJp-0GQQzCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#garder les colonnes qui contiennent au moins 90% de données valides\n",
        "\n",
        "tmp = ames[newfeaturelist]\n",
        "good_cols = tmp.count() > 0.9*len(tmp)\n",
        "\n",
        "goodfeatures = good_cols.index[good_cols == True].to_list()\n",
        "\n",
        "newfeaturelist = list(set(goodfeatures)) #on s'assure de ne pas avoir de doublons en convertissant temporairement en set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuMmjKHDQYqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = ames[newfeaturelist+[\"SalePrice\"]]\n",
        "dataset = dataset.fillna(0)\n",
        "\n",
        "X = dataset[newfeaturelist]\n",
        "\n",
        "y = dataset[\"SalePrice\"]\n",
        "\n",
        "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.4, random_state=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oM1lPqJiVF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from catboost import Pool, CatBoostRegressor\n",
        "\n",
        "\n",
        "cat_feature_ids = [i for i in range(len(newfeaturelist)) if newfeaturelist[i] in catfeatures]\n",
        "\n",
        "\n",
        "train_pool = Pool(Xtrain.values, ytrain.values, cat_features=cat_feature_ids)\n",
        "val_pool = Pool(Xval, yval, cat_features=cat_feature_ids) \n",
        "all_pool = Pool(X, y, cat_features=cat_feature_ids) \n",
        "\n",
        "# specify the training parameters \n",
        "model = CatBoostRegressor(iterations=300, \n",
        "                          depth=3, \n",
        "                          learning_rate=0.2, \n",
        "                          loss_function='RMSE')\n",
        "#train the model\n",
        "model.fit(train_pool, silent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac894-uU00ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make the prediction using the resulting model\n",
        "trainpreds = model.predict(train_pool)\n",
        "valpreds = model.predict(val_pool)\n",
        "\n",
        "rmselog(trainpreds,ytrain.values), rmselog(valpreds,yval.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pS2SzLIXCmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify the training parameters \n",
        "model = CatBoostRegressor(iterations=300, \n",
        "                          depth=3, \n",
        "                          learning_rate=0.2, \n",
        "                          loss_function='RMSE')\n",
        "\n",
        "#entraînement du modèle, avec un critère d'arrêt lorsque les performances en validation baissent: éviter le surentraînement (overfitting)\n",
        "model.fit(train_pool,eval_set = val_pool, early_stopping_rounds = 100, silent = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOJRK7daRgy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make the prediction using the resulting model\n",
        "trainpreds = model.predict(train_pool)\n",
        "valpreds = model.predict(val_pool)\n",
        "\n",
        "#Pour éviter de prédire des valeurs anormales, on limite les prédictions au range du set d'entraînement\n",
        "trainpreds = trainpreds.clip(ytrain.min(), ytrain.max())\n",
        "valpreds = valpreds.clip(ytrain.min(), ytrain.max())\n",
        "\n",
        "rmselog(trainpreds,ytrain.values), rmselog(valpreds,yval.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IryInRS1ZZna",
        "colab_type": "text"
      },
      "source": [
        "### Soumission des résultats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlG-EhJ8ZM77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset = pd.read_csv(\"https://raw.githubusercontent.com/titsitits/Python_Data_Science/master/Donn%C3%A9es/test.csv\")\n",
        "\n",
        "Xtest = testset[newfeaturelist].fillna(0)\n",
        "test_pool = Pool(Xtest.values, cat_features=cat_feature_ids)\n",
        "\n",
        "#Pour la soumission, on peut éventuellement réentraîner le modèle sur toutes les données d'entraînement et de validation (pour espérer avoir un modèle plus générique. Cependant: attention à l'overfitting sans critère d'arrêt)\n",
        "model.fit(all_pool, silent = True)\n",
        "\n",
        "#Prédictions\n",
        "testpreds = model.predict(test_pool)\n",
        "\n",
        "submission = testset\n",
        "submission[\"SalePrice\"] = testpreds\n",
        "submission = submission[[\"Id\",\"SalePrice\"]]\n",
        "submission.to_csv(\"mysubmission_catboost.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMqBzDv9gMzb",
        "colab_type": "text"
      },
      "source": [
        "Les résultats avec le fichier de baseline (sample_submission.csv) ont été obtenus avec une régression linéaire sur l'année et le mosi de vente, la surface du lot et le nombre de chambres. Les résultats de la soumission sur Kaggle donnent: \n",
        "\n",
        "`rmselog = 0.40890`\n",
        "\n",
        "La soumission avec catboost sur ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'MSZoning', 'PoolQC', 'SaleType', 'SaleCondition', 'LotShape'] donne\n",
        "\n",
        "`rmselog = 0.25360`\n",
        "\n",
        "L'algorithme proposé prédit donc mieux les maisons que la baseline.\n",
        "\n",
        "Bien évidemment, ce n'est qu'un essai, et il existe de très nombreuses possibilités pour améliorer le modèle:\n",
        "\n",
        "* Vérifier les données et supprimer d'éventuelles anomalies\n",
        "* Tester d'autre ensembles de variables (en explorant plus profondément leurs relations et leur influence sur le prix)\n",
        "* Créer de nouvelles variables pertinentes. Par exemple, à partir de la variable catégorielle \"neighboorhood\" et du prix moyen de chaque quartier, on pourrait extraire une variable numérique indiquant le \"standing\" du quartier. On pourrait aussi traduire les nombreuses variables indiquant une échelle de qualité (\"Excellent\",\"Typical\",\"Fair\", ...) en variables numériques. On pourrait également extraire des variables \"dummy\" sur d'autres catégories.\n",
        "* Les variables sont peut-être redondantes (elles apportent la même information), ce qui complexifie inutilement le modèle. Il peut être intéressant d'extraire un ensemble de variables apportant un maximum d'informations non-redondantes.\n",
        "* Tester d'autres algorithmes de machine learning\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHIuocV6ieV1",
        "colab_type": "text"
      },
      "source": [
        "## Bonus - Exploration de la redondance\n",
        "\n",
        "L'analyse de corrélation permet de sélectionner facilement des variables prédictives pertinentes, comme nous l'avons vu plus haut.\n",
        "Une limitation importante de cette méthode de sélection de variables est la non-prise en compte de la redondance possible entre les variables prédictives. Deux variables redondantes signifie qu'elles contiennent en partie la même information (elles sont donc généralement fortement corrélées entre elles). La sélection d'une variable redondante apporte donc peu d'information supplémentaire pour l'entraînement du modèle prédictif. au contraire, l'augmentation du nombre de variables rend alors le modèle inutilement plus complexe, ce qui rend l'entraînement plus lourd, et augmente les risques d'overfitting.\n",
        "\n",
        "Dans les exemples proposés ci-dessous, nous avons notamment utilisé deux variables particulièrement redondantes: 'GarageCars' et 'GarageArea', qui apportent presque la même information (la surface qu garage vs le nombre de voitures qu'on peut y mettre). On peut d'ailleurs remarquer une très forte corrélation entre ces deux variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqNwDtqElZVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ames[[\"GarageCars\",\"GarageArea\"]].corr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtGKe5W0ljEc",
        "colab_type": "text"
      },
      "source": [
        "Lors de l'analyse de corrélation, il est donc également intéressant de vérifier que les variables sélectionnées ne soient donc pas redondantes. On peut notamment s'attendre à ce que les années de construction de la maison et des garages soient redondantes, tout comme différentes variables liées à la surface de la maison ou au nombre de pièces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BqyPdVlie6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def plot_corr(df, absolute = False, size = (15,15), decimal = 1):\n",
        "  \n",
        "  correlation_matrix = df.corr().round(decimal)\n",
        "  if absolute:\n",
        "    correlation_matrix = correlation_matrix.abs()\n",
        "  plt.figure(figsize = size)\n",
        "  sns.heatmap(correlation_matrix, annot=True)\n",
        "             \n",
        "plot_corr(ames, absolute = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fyM4nHLmCb7",
        "colab_type": "text"
      },
      "source": [
        "### Bonus - L'analyse en composantes principales (PCA) \n",
        "\n",
        "Une manière de se débarrasser du problème est l'utilisation de l'analyse en composantes principales (PCA, aussi appelée Singular value Decomposition - SVD). C'est un algorithme d'algèbre matriciel (souvent considéré comme une technique de machine learning non-supervisé), permettant d'extraire un nouvel ensemble de variables **orthogonales** (i.e. non-corrélées entre elles), à partir de combinaisons linéaires des variables d'origine. Dans ce nouvel ensemble de variables, on peut alors sélectionner les variables prédictives sans le moindre risque de redondance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyushFQudbFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "dataset = ames[featurelist+[\"SalePrice\"]]\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "X = dataset[featurelist]\n",
        "y = dataset[\"SalePrice\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
        "\n",
        "#On réalise la PCA sur les données d'entraînement\n",
        "pca = PCA(n_components=X.shape[1]-2)\n",
        "pca.fit(Xtrain)\n",
        "Xtrainpcs = pca.transform(Xtrain)\n",
        "Xtrainpcs = pd.DataFrame(Xtrainpcs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYQO69sQp0P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_corr(Xtrainpcs, absolute = True, size = (10,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgIOHAp-quZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrainpcs[\"SalePrice\"] = ytrain\n",
        "\n",
        "correlation_matrix = Xtrainpcs.corr()\n",
        "corr_with_price = correlation_matrix[\"SalePrice\"].abs()\n",
        "best_pcs = corr_with_price.sort_values(ascending=False)\n",
        "\n",
        "# On ne évidemment peut utiliser la label comme variable prédictive\n",
        "best_pcs = best_pcs.drop(\"SalePrice\")\n",
        "\n",
        "best_pcs.plot.bar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMMZwgD5rkZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pcslist = best_pcs.index[:25].to_list()\n",
        "\n",
        "Xpcs = pca.transform(X)\n",
        "Xpcs = pd.DataFrame(Xpcs)\n",
        "Xfeatures = Xpcs[pcslist]\n",
        "\n",
        "Xtrain, Xval, ytrain, yval = train_test_split(Xfeatures, y, test_size = 0.3)\n",
        "\n",
        "#Entraînement\n",
        "price_predictor.fit(Xtrain, ytrain)\n",
        "\n",
        "#Prédictions\n",
        "trainpreds = price_predictor.predict(Xtrain)\n",
        "valpreds = price_predictor.predict(Xval)\n",
        "\n",
        "#Pour éviter de prédire des valeurs anormales, on limite les prédictions au range du set d'entraînement\n",
        "trainpreds = trainpreds.clip(ytrain.min(), ytrain.max())\n",
        "valpreds = valpreds.clip(ytrain.min(), ytrain.max())\n",
        "\n",
        "rmselog(trainpreds, ytrain.values) , rmselog(valpreds, yval.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps699u0CxwDc",
        "colab_type": "text"
      },
      "source": [
        "### Bonus - PLSRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaAA_7EjxxvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "pls_predictor = PLSRegression(max_iter=200, n_components=20)\n",
        "\n",
        "dataset = ames[featurelist+[\"SalePrice\"]]\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "X = dataset[featurelist]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "y = dataset[\"SalePrice\"]\n",
        "\n",
        "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.4)\n",
        "pls_predictor.fit(Xtrain, ytrain)\n",
        "\n",
        "#Prédictions\n",
        "trainpreds = pls_predictor.predict(Xtrain)\n",
        "valpreds = pls_predictor.predict(Xval)\n",
        "\n",
        "#Pour éviter de prédire des valeurs anormales, on limite les prédictions au range du set d'entraînement\n",
        "trainpreds = trainpreds.clip(ytrain.min(), ytrain.max())\n",
        "valpreds = valpreds.clip(ytrain.min(), ytrain.max())\n",
        "\n",
        "rmselog(trainpreds, ytrain.values) , rmselog(valpreds, yval.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2lUJu-Do6vJ",
        "colab_type": "text"
      },
      "source": [
        "# Poubelle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTuy_trxo79x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#optional: add polynomials (only to continuous variables)\n",
        "poly = False\n",
        "if poly:\n",
        "  goodcontfeatures = [g for g in goodfeatures if g in contfeatures]\n",
        "  Xcont = dataset[goodcontfeatures]\n",
        "  Xcont = add_polynomials(Xcont)\n",
        "  goodcatfeatures = [g for g in goodfeatures if g in catfeatures]\n",
        "  Xcat = dataset[goodcatfeatures]\n",
        "  X = pd.concat([Xcont, Xcat], axis=1)\n",
        "  newfeaturelist = X.columns"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}